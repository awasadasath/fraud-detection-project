{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc122c9c-90b1-4b1b-a10d-ef280f9e272f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PART 1: ENVIRONMENT SETUP & INGESTION\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pyspark.sql.functions import col, lit, when, current_date, max as spark_max\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Initializing Fraud Detection Pipeline\")\n",
    "\n",
    "# [SECURITY BEST PRACTICE]\n",
    "# In a production environment, never hardcode keys. \n",
    "# Use Databricks Secrets or IAM Roles for authentication.\n",
    "# Example: dbutils.secrets.get(scope=\"gcp-scope\", key=\"gcp-service-account\")\n",
    "\n",
    "# Simulating data loading from a secure Data Lake source\n",
    "# Source: Google Cloud Storage (GCS) -> Databricks\n",
    "FILE_PATH = \"dbfs:/FileStore/tables/paysim.csv\" \n",
    "\n",
    "print(f\"Reading data from: {FILE_PATH}\")\n",
    "# Using Spark native reader for scalability\n",
    "df = spark.read.csv(FILE_PATH, header=True, inferSchema=True)\n",
    "\n",
    "# Note: For this portfolio demonstration, we assume data is loaded into 'df'\n",
    "print(\"Ingestion successful.\")\n",
    "\n",
    "print(\"Previewing Data:\")\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01de61e9-edcd-47f1-bae1-579107d2f77d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PART 2: BRONZE LAYER (RAW PERSISTENCE)\n",
    "\n",
    "print(\"Step 1: Creating Bronze Layer\")\n",
    "\n",
    "# 1. Schema Validation\n",
    "print(\"Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "# 2. Write to Delta Lake\n",
    "# We use Delta format to ensure ACID transactions and reliability.\n",
    "print(\"Saving Bronze Table: 'paysim_bronze'\")\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"paysim_bronze\")\n",
    "\n",
    "print(f\"Bronze Layer Created. Total Rows: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b1172c-e45c-4d0a-97de-bbf2d919617c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PART 3: SILVER LAYER (CLEANSED & QUALITY CHECK)\n",
    "\n",
    "print(\"Step 2: Creating Silver Layer\")\n",
    "\n",
    "# 1. Read from Bronze Delta Table\n",
    "df_bronze = spark.read.table(\"paysim_bronze\")\n",
    "\n",
    "# 2. Define Data Quality Rules\n",
    "# Logic: Fraud only happens in TRANSFER/CASH_OUT and Amount must be positive.\n",
    "dq_condition = (\n",
    "    (col(\"amount\") >= 0) & \n",
    "    (col(\"type\").isin(\"TRANSFER\", \"CASH_OUT\"))\n",
    ")\n",
    "\n",
    "# 3. Filter Good vs Bad Data (Quarantine)\n",
    "df_good = df_bronze.filter(dq_condition)\n",
    "df_bad = df_bronze.filter(~dq_condition).withColumn(\"dq_issue\", lit(\"Invalid Type or Negative Amount\"))\n",
    "\n",
    "# 4. Type Casting for Performance\n",
    "df_silver = df_good.withColumn(\"amount\", col(\"amount\").cast(\"double\")) \\\n",
    "                   .withColumn(\"oldbalanceOrg\", col(\"oldbalanceOrg\").cast(\"double\")) \\\n",
    "                   .withColumn(\"newbalanceOrig\", col(\"newbalanceOrig\").cast(\"double\")) \\\n",
    "                   .withColumn(\"oldbalanceDest\", col(\"oldbalanceDest\").cast(\"double\")) \\\n",
    "                   .withColumn(\"newbalanceDest\", col(\"newbalanceDest\").cast(\"double\")) \\\n",
    "                   .withColumn(\"isFraud\", col(\"isFraud\").cast(\"integer\"))\n",
    "\n",
    "# 5. Write to Silver Delta Tables\n",
    "print(\"Saving Silver Table: 'paysim_silver'\")\n",
    "df_silver.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"paysim_silver\")\n",
    "\n",
    "print(\"Saving Quarantine Table: 'paysim_quarantine'\")\n",
    "df_bad.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"paysim_quarantine\")\n",
    "\n",
    "print(f\"Stats - Silver: {df_silver.count():,} | Quarantine: {df_bad.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "975f8dab-d645-4bff-9aca-47d6265b4be1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PART 4: GOLD LAYER (FEATURE ENGINEERING)\n",
    "\n",
    "print(\"Step 3: Creating Gold Layer\")\n",
    "\n",
    "df_silver = spark.read.table(\"paysim_silver\")\n",
    "\n",
    "# --- A: Feature Engineering ---\n",
    "# Adding critical behavioral features for ML:\n",
    "# 1. ErrorBalance: Discrepancy in transaction balancing.\n",
    "# 2. HourOfDay: Time-based patterns.\n",
    "# 3. AmountRatio: Ratio of transaction amount to original balance (Account emptying behavior).\n",
    "\n",
    "df_gold = df_silver.withColumn(\"errorBalanceOrig\", col(\"newbalanceOrig\") + col(\"amount\") - col(\"oldbalanceOrg\")) \\\n",
    "                   .withColumn(\"errorBalanceDest\", col(\"oldbalanceDest\") + col(\"amount\") - col(\"newbalanceDest\")) \\\n",
    "                   .withColumn(\"type_index\", when(col(\"type\") == \"TRANSFER\", 0).otherwise(1)) \\\n",
    "                   .withColumn(\"hourOfDay\", col(\"step\") % 24) \\\n",
    "                   .withColumn(\"amountRatio\", col(\"amount\") / (col(\"oldbalanceOrg\") + 0.001))\n",
    "\n",
    "# Select Final Schema\n",
    "final_columns = [\n",
    "    \"step\", \"type_index\", \"amount\", \"amountRatio\", \"hourOfDay\",\n",
    "    \"oldbalanceOrg\", \"newbalanceOrig\", \"errorBalanceOrig\",\n",
    "    \"oldbalanceDest\", \"newbalanceDest\", \"errorBalanceDest\",\n",
    "    \"isFraud\"\n",
    "]\n",
    "df_gold_ml = df_gold.select(final_columns)\n",
    "\n",
    "# --- B: Risk Profile (Customer Dimension) ---\n",
    "# Identify high-risk customers based on history\n",
    "df_risk_profile = df_silver.groupBy(\"nameOrig\").agg(spark_max(\"amount\").alias(\"max_txn_amount\")) \\\n",
    "    .withColumn(\"risk_level\", when(col(\"max_txn_amount\") > 1000000, \"High\").otherwise(\"Low\")) \\\n",
    "    .withColumn(\"effective_date\", current_date())\n",
    "\n",
    "# Persist Gold Tables\n",
    "print(\"Saving Gold Table (ML Ready): 'paysim_gold'\")\n",
    "df_gold_ml.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"paysim_gold\")\n",
    "\n",
    "print(\"Saving Risk Profile: 'dim_customer_risk'\")\n",
    "df_risk_profile.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dim_customer_risk\")\n",
    "\n",
    "print(\"Gold Layer Created Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcbcf9ca-a8a8-4e34-959c-e1a7ef8ab661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PART 5: MACHINE LEARNING & EVALUATION\n",
    "\n",
    "print(\"Step 4: Training Machine Learning Model\")\n",
    "\n",
    "# 1. Load Gold Data\n",
    "df_gold = spark.read.table(\"paysim_gold\")\n",
    "\n",
    "# 2. Vector Assembler\n",
    "feature_cols = [\n",
    "    \"type_index\", \"amount\", \"amountRatio\", \"hourOfDay\",\n",
    "    \"oldbalanceOrg\", \"newbalanceOrig\", \"errorBalanceOrig\",\n",
    "    \"oldbalanceDest\", \"newbalanceDest\", \"errorBalanceDest\"\n",
    "]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_ready = assembler.transform(df_gold)\n",
    "\n",
    "# --- Resource Optimization: 10% Sampling ---\n",
    "# Utilizing 10% stratified sampling to optimize training time on limited resources \n",
    "# while maintaining statistical significance.\n",
    "print(\"Downsampling data to 10% for optimized training\")\n",
    "df_sampled = df_ready.sample(withReplacement=False, fraction=0.10, seed=1234)\n",
    "\n",
    "# 3. Train/Test Split\n",
    "train_data, test_data = df_sampled.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# 4. Train Model (Random Forest)\n",
    "rf = RandomForestClassifier(labelCol=\"isFraud\", featuresCol=\"features\", numTrees=10)\n",
    "print(\"Training Random Forest model...\")\n",
    "model = rf.fit(train_data)\n",
    "print(\"Training Complete.\")\n",
    "\n",
    "# 5. Prediction & Standard Evaluation\n",
    "predictions = model.transform(test_data)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"isFraud\", predictionCol=\"prediction\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "f1_score = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"\\n--- General Metrics ---\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"F1-Score:  {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3df4b095-acdf-472e-81fd-18be52bf986e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PART 6: FRAUD METRICS (CLASS 1)\n",
    "\n",
    "print(\"Calculation Logic: Focus ONLY on Fraud (Class 1)\")\n",
    "\n",
    "# Extracting Confusion Matrix counts manually\n",
    "counts = predictions.groupBy(\"isFraud\", \"prediction\").count().collect()\n",
    "\n",
    "tp = 0; fp = 0; fn = 0\n",
    "\n",
    "for row in counts:\n",
    "    if row['isFraud'] == 1 and row['prediction'] == 1.0: tp = row['count']\n",
    "    elif row['isFraud'] == 0 and row['prediction'] == 1.0: fp = row['count']\n",
    "    elif row['isFraud'] == 1 and row['prediction'] == 0.0: fn = row['count']\n",
    "\n",
    "# Calculate Metrics\n",
    "try:\n",
    "    recall_fraud = tp / (tp + fn)       # Catch Rate\n",
    "    precision_fraud = tp / (tp + fp)    # Accuracy of Catch\n",
    "except ZeroDivisionError:\n",
    "    recall_fraud = 0; precision_fraud = 0\n",
    "\n",
    "print(\"\\n --- Real Performance on FRAUD Class (Class 1) ---\")\n",
    "print(f\" Recall (Catch Rate):    {recall_fraud:.4f} ({recall_fraud*100:.2f}%)\")\n",
    "print(f\" Precision (Accuracy):  {precision_fraud:.4f} ({precision_fraud*100:.2f}%)\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Summary: Caught {tp} out of {tp+fn} fraudsters.\")\n",
    "print(f\"False Positives (Wrongly Accused): {fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b72b2148-fee5-4d1a-a489-99897be7974b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PART 7: FEATURE IMPORTANCE VISUALIZATION\n",
    "\n",
    "# Extract Feature Importance\n",
    "importances = model.featureImportances\n",
    "feature_list = []\n",
    "for i, col_name in enumerate(feature_cols):\n",
    "    feature_list.append((col_name, float(importances[i])))\n",
    "\n",
    "df_importance = pd.DataFrame(feature_list, columns=[\"Feature\", \"Importance\"])\n",
    "df_importance = df_importance.sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "# Plot Graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=df_importance, palette=\"viridis\")\n",
    "plt.title('Top Factors Driving Fraud Detection', fontsize=15)\n",
    "plt.xlabel('Importance Score')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8851590791735036,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "paysim_project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
