{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc122c9c-90b1-4b1b-a10d-ef280f9e272f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PART 1: INGESTION\n",
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "# CONFIGURATION\n",
    "DBFS_KEY_PATH = \"dbfs:/FileStore/tables/gcp_key.json\" \n",
    "\n",
    "BUCKET_NAME = \"paysim-datalake\"\n",
    "FILE_NAME = \"paysim.csv\"\n",
    "\n",
    "# Path ชั่วคราวบน Driver (Local)\n",
    "LOCAL_KEY_PATH = \"/tmp/gcp_key.json\"\n",
    "LOCAL_DATA_PATH = f\"/tmp/{FILE_NAME}\"\n",
    "# Path บน DBFS (เพื่อให้ Spark อ่านได้)\n",
    "DBFS_DATA_PATH = f\"dbfs:/FileStore/raw/{FILE_NAME}\"\n",
    "\n",
    "print(\"Step 1: Secure Ingestion\")\n",
    "\n",
    "try:\n",
    "    # 1. Setup Credentials (Copy Key from DBFS -> Local)\n",
    "    dbutils.fs.cp(DBFS_KEY_PATH, f\"file:{LOCAL_KEY_PATH}\")\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = LOCAL_KEY_PATH\n",
    "    print(\"Credentials Loaded.\")\n",
    "\n",
    "    # 2. Download Data from GCS to Local Driver\n",
    "    print(f\"Downloading {FILE_NAME}\")\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "    blob = bucket.blob(FILE_NAME)\n",
    "    blob.download_to_filename(LOCAL_DATA_PATH)\n",
    "    \n",
    "    # 3. Move Data to DBFS (เพื่อให้ใช้ spark.read ได้)\n",
    "    dbutils.fs.mv(f\"file:{LOCAL_DATA_PATH}\", DBFS_DATA_PATH)\n",
    "    \n",
    "    # 4. Read with Spark\n",
    "    print(\"Reading CSV with Spark\")\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(DBFS_DATA_PATH)\n",
    "    \n",
    "    print(\"Ingestion successful.\")\n",
    "\n",
    "    # Preview Data\n",
    "    print(\"Previewing Data:\")\n",
    "    display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01de61e9-edcd-47f1-bae1-579107d2f77d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PART 2: BRONZE LAYER\n",
    "    print(\"Step 1: Creating Bronze Layer\")\n",
    "    \n",
    "    # 1. Check Schema\n",
    "    print(\"Schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    # 2. Count Rows\n",
    "    print(f\"Total Rows: {df.count():,}\")\n",
    "    \n",
    "    # 3. Save as Delta Table\n",
    "    print(\"Creating Bronze Table: 'paysim_bronze'\")\n",
    "    spark.sql(\"DROP TABLE IF EXISTS paysim_bronze\")\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"paysim_bronze\")\n",
    "    \n",
    "    print(\"Bronze Layer Created\")\n",
    "    \n",
    "    # Cleanup Local Key\n",
    "    if os.path.exists(LOCAL_KEY_PATH): os.remove(LOCAL_KEY_PATH)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    if os.path.exists(LOCAL_KEY_PATH): os.remove(LOCAL_KEY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b1172c-e45c-4d0a-97de-bbf2d919617c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PART 3: SILVER LAYER (CLEANSED & QUALITY CHECK)\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "print(\"Step 2: Creating Silver Layer\")\n",
    "\n",
    "# 1. Read from Bronze Delta\n",
    "df_bronze = spark.table(\"paysim_bronze\")\n",
    "\n",
    "# Step A: Data Quality Check\n",
    "# Rule: Amount must be non-negative. Negative values indicate system errors.\n",
    "dq_condition = col(\"amount\") >= 0\n",
    "\n",
    "# Separate Bad Data (Quarantine) from \"Valid Data\"\n",
    "df_valid_amount = df_bronze.filter(dq_condition)\n",
    "df_quarantine = df_bronze.filter(~dq_condition).withColumn(\"dq_issue\", lit(\"Negative Amount\"))\n",
    "\n",
    "# Step B: Business Scope Separation\n",
    "# Rule: We strictly focus on TRANSFER and CASH_OUT for fraud detection.\n",
    "# Other types (PAYMENT, DEBIT) are valid but out of scope for this model.\n",
    "scope_condition = col(\"type\").isin(\"TRANSFER\", \"CASH_OUT\")\n",
    "\n",
    "# 1. Target Data (Silver): Ready for ML processing\n",
    "df_silver = df_valid_amount.filter(scope_condition)\n",
    "\n",
    "# 2. Out-of-Scope Data (Others): Archived for future analytics\n",
    "df_others = df_valid_amount.filter(~scope_condition)\n",
    "\n",
    "# 4. Type Casting for Performance (Applied to Silver Table)\n",
    "# Casting String to Double/Integer for efficient calculation\n",
    "df_silver = df_silver.withColumn(\"amount\", col(\"amount\").cast(\"double\")) \\\n",
    "                   .withColumn(\"oldbalanceOrg\", col(\"oldbalanceOrg\").cast(\"double\")) \\\n",
    "                   .withColumn(\"newbalanceOrig\", col(\"newbalanceOrig\").cast(\"double\")) \\\n",
    "                   .withColumn(\"oldbalanceDest\", col(\"oldbalanceDest\").cast(\"double\")) \\\n",
    "                   .withColumn(\"newbalanceDest\", col(\"newbalanceDest\").cast(\"double\")) \\\n",
    "                   .withColumn(\"isFraud\", col(\"isFraud\").cast(\"integer\"))\n",
    "\n",
    "# 5. Save as Delta Tables\n",
    "print(\"Creating Silver Table (Target): 'paysim_silver'\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS paysim_silver\")\n",
    "df_silver.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"paysim_silver\")\n",
    "\n",
    "print(\"Creating Others Table (Out of Scope): 'paysim_others'\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS paysim_others\")\n",
    "df_others.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"paysim_others\")\n",
    "\n",
    "print(\"Creating Quarantine Table (Bad Data): 'paysim_quarantine'\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS paysim_quarantine\")\n",
    "df_quarantine.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"paysim_quarantine\")\n",
    "\n",
    "# Print Statistics\n",
    "print(f\"\"\"\n",
    "Stats Summary:\n",
    "- Silver (ML Ready):   {spark.table('paysim_silver').count():,} rows\n",
    "- Others (Ignored):    {spark.table('paysim_others').count():,} rows\n",
    "- Quarantine (Bad):    {spark.table('paysim_quarantine').count():,} rows\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "975f8dab-d645-4bff-9aca-47d6265b4be1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PART 4: GOLD LAYER (FEATURES & RISK)\n",
    "from pyspark.sql.functions import col, when, current_date, lit, max as spark_max\n",
    "\n",
    "print(\"Step 3: Creating Gold Layer\")\n",
    "\n",
    "# 1. Read from Silver Delta\n",
    "df_silver = spark.table(\"paysim_silver\")\n",
    "\n",
    "# A: Feature Engineering\n",
    "# We are adding 3 critical features for fraud detection:\n",
    "# 1. errorBalance: Detects mathematical anomalies in account balances.\n",
    "# 2. hourOfDay: Fraud often happens at specific hours (extracted from step).\n",
    "# 3. amountRatio: Fraudsters tend to empty accounts (amount / oldBalance).\n",
    "\n",
    "df_gold = df_silver.withColumn(\"errorBalanceOrig\", col(\"newbalanceOrig\") + col(\"amount\") - col(\"oldbalanceOrg\")) \\\n",
    "                   .withColumn(\"errorBalanceDest\", col(\"oldbalanceDest\") + col(\"amount\") - col(\"newbalanceDest\")) \\\n",
    "                   .withColumn(\"type_index\", when(col(\"type\") == \"TRANSFER\", 0).otherwise(1)) \\\n",
    "                   .withColumn(\"hourOfDay\", col(\"step\") % 24) \\\n",
    "                   .withColumn(\"amountRatio\", col(\"amount\") / (col(\"oldbalanceOrg\") + 0.001))\n",
    "\n",
    "# Select Final Columns for ML\n",
    "final_columns = [\n",
    "    \"step\", \n",
    "    \"type_index\", \n",
    "    \"amount\", \n",
    "    \"amountRatio\",      \n",
    "    \"hourOfDay\",      \n",
    "    \"oldbalanceOrg\", \n",
    "    \"newbalanceOrig\", \n",
    "    \"errorBalanceOrig\",\n",
    "    \"oldbalanceDest\", \n",
    "    \"newbalanceDest\", \n",
    "    \"errorBalanceDest\",\n",
    "    \"isFraud\"\n",
    "]\n",
    "\n",
    "df_gold_ml = df_gold.select(final_columns)\n",
    "\n",
    "print(\"Creating Gold Table: 'paysim_gold'\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS paysim_gold\")\n",
    "df_gold_ml.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"paysim_gold\")\n",
    "\n",
    "# B: Risk Profile (Customer Behavioral Snapshot)\n",
    "# Identify high-risk customers based on max transaction history.\n",
    "# Strategy: Daily Full Refresh (Snapshot) to update the latest status for ALL customers.\n",
    "\n",
    "df_risk_profile = df_silver.groupBy(\"nameOrig\") \\\n",
    "    .agg(spark_max(\"amount\").alias(\"max_txn_amount\")) \\\n",
    "    .withColumn(\"risk_level\", when(col(\"max_txn_amount\") > 1000000, \"High\").otherwise(\"Low\")) \\\n",
    "    .withColumn(\"effective_date\", current_date()) \\\n",
    "    .withColumn(\"is_current\", lit(True))\n",
    "\n",
    "# Write to Delta Lake (Overwrite mode = Always keep the latest state)\n",
    "# Note: We save ALL customers (Low & High) to maintain a complete Dimension Table, not just a blacklist.\n",
    "df_risk_profile.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dim_customer_risk\")\n",
    "\n",
    "# For demonstration, display only HIGH RISK customers\n",
    "print(\"Displaying Sample of HIGH RISK Customers:\")\n",
    "display(df_risk_profile.filter(col(\"risk_level\") == \"High\"))\n",
    "\n",
    "print(\"Gold Layer Created Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcbcf9ca-a8a8-4e34-959c-e1a7ef8ab661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PART 5: MACHINE LEARNING PIPELINE & EVALUATION\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Starting ML Pipeline processing...\")\n",
    "\n",
    "# 1. Load Data\n",
    "df_gold = spark.table(\"paysim_gold\")\n",
    "\n",
    "# 2. Data Splitting (Temporal Split)\n",
    "# Train: First 600 hours (~25 days)\n",
    "# Test:  Remaining hours (Future data)\n",
    "split_step = 600\n",
    "train_raw = df_gold.filter(col(\"step\") <= split_step)\n",
    "test_raw = df_gold.filter(col(\"step\") > split_step)\n",
    "\n",
    "# 3. Strategic Sampling\n",
    "\n",
    "# A. Training Set: Majority Undersampling\n",
    "# Logic: Keep 100% of Fraud, Sample 10% of Normal.\n",
    "print(\"Preparing Training Data (Undersampling Normal class)\")\n",
    "train_data = train_raw.filter(col(\"isFraud\") == 1).union(\n",
    "    train_raw.filter(col(\"isFraud\") == 0).sample(False, 0.10, seed=1234)\n",
    ")\n",
    "\n",
    "# B. Test Set: Full Real-World Data\n",
    "print(\"Preparing Test Data...\")\n",
    "test_data = test_raw\n",
    "\n",
    "# Display counts\n",
    "print(f\"Training Set Count: {train_data.count():,}\")\n",
    "print(f\"Test Set Count:     {test_data.count():,}\")\n",
    "\n",
    "# 4. Model Pipeline Construction\n",
    "\n",
    "feature_cols = [\n",
    "    \"type_index\", \"amount\", \"amountRatio\", \"hourOfDay\",\n",
    "    \"oldbalanceOrg\", \"newbalanceOrig\", \"errorBalanceOrig\",\n",
    "    \"oldbalanceDest\", \"newbalanceDest\", \"errorBalanceDest\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# RandomForest Configuration\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"isFraud\", \n",
    "    featuresCol=\"features\", \n",
    "    numTrees=20,  \n",
    "    maxDepth=10, \n",
    "    seed=1234\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# 5. Training\n",
    "print(\"Training Random Forest Model...\")\n",
    "pipeline_model = pipeline.fit(train_data)\n",
    "print(\"Training Complete.\")\n",
    "\n",
    "# 6. Prediction\n",
    "print(\"Predicting on Test Data...\")\n",
    "predictions = pipeline_model.transform(test_data)\n",
    "\n",
    "# 7. Show Full Confusion Matrix\n",
    "print(\"Confusion Matrix Table:\"\n",
    "actual_counts = predictions.groupBy(\"isFraud\", \"prediction\").count()\n",
    "\n",
    "# Create template with all 4 possibilities\n",
    "template_data = [\n",
    "    (1, 1.0), # TP\n",
    "    (0, 1.0), # FP\n",
    "    (1, 0.0), # FN\n",
    "    (0, 0.0)  # TN\n",
    "]\n",
    "template_df = spark.createDataFrame(template_data, [\"isFraud\", \"prediction\"])\n",
    "\n",
    "# Join template with actual counts\n",
    "full_matrix = template_df.join(actual_counts, on=[\"isFraud\", \"prediction\"], how=\"left\") \\\n",
    "                         .na.fill(0) \\\n",
    "                         .orderBy(col(\"isFraud\").desc(), col(\"prediction\").desc())\n",
    "\n",
    "full_matrix.show()\n",
    "\n",
    "# 8. Calculate Manual Metrics\n",
    "# Use collected rows for metrics calculation\n",
    "conf_matrix = full_matrix.collect()\n",
    "\n",
    "tp = fp = fn = tn = 0\n",
    "for row in conf_matrix:\n",
    "    if row['isFraud'] == 1 and row['prediction'] == 1.0: tp = row['count']\n",
    "    elif row['isFraud'] == 0 and row['prediction'] == 1.0: fp = row['count']\n",
    "    elif row['isFraud'] == 1 and row['prediction'] == 0.0: fn = row['count']\n",
    "    elif row['isFraud'] == 0 and row['prediction'] == 0.0: tn = row['count']\n",
    "\n",
    "try:\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "except:\n",
    "    precision = recall = f1 = accuracy = 0\n",
    "\n",
    "# 9. Final Report\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL MODEL PERFORMANCE REPORT\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Accuracy:  {accuracy:.2%}\")\n",
    "print(f\"Precision: {precision:.2%}\")\n",
    "print(f\"Recall:    {recall:.2%}\")\n",
    "print(f\"F1-Score:  {f1:.2%}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"True Positives (Caught Fraud): {tp}\")\n",
    "print(f\"False Negatives (Missed Fraud): {fn}\")\n",
    "print(f\"False Positives (False Alarm):  {fp}\")\n",
    "print(f\"True Negatives (Correct Normal): {tn}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "\n",
    "# PART 6: OVERFITTING CHECK (TRAIN vs TEST)\n",
    "print(\"OVERFITTING CHECK (TRAIN vs TEST)\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "def quick_evaluate(dataset, name):\n",
    "    print(f\"Evaluating on {name}\")\n",
    "    pred = pipeline_model.transform(dataset)\n",
    "    \n",
    "    # Use Spark GroupBy\n",
    "    metrics = pred.groupBy(\"isFraud\", \"prediction\").count().collect()\n",
    "    \n",
    "    tp = fp = fn = tn = 0\n",
    "    for row in metrics:\n",
    "        if row['isFraud'] == 1 and row['prediction'] == 1.0: tp = row['count']\n",
    "        elif row['isFraud'] == 0 and row['prediction'] == 1.0: fp = row['count']\n",
    "        elif row['isFraud'] == 1 and row['prediction'] == 0.0: fn = row['count']\n",
    "        elif row['isFraud'] == 0 and row['prediction'] == 0.0: tn = row['count']\n",
    "    \n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    print(f\"Recall:    {recall:.2%}\")\n",
    "    print(f\"Precision: {precision:.2%}\")\n",
    "    return recall\n",
    "\n",
    "# 1. Check Training Set\n",
    "train_recall = quick_evaluate(train_data, \"TRAINING SET\")\n",
    "\n",
    "# 2. Check Test Set\n",
    "test_recall = quick_evaluate(test_data, \"TEST SET\")\n",
    "\n",
    "# 3. Compare Results\n",
    "gap = train_recall - test_recall\n",
    "print(\"-\" * 50)\n",
    "print(f\"GAP ANALYSIS (Train - Test): {gap:.2%}\")\n",
    "\n",
    "if gap > 0.15:\n",
    "    print(\"High Overfitting Risk\")\n",
    "    print(\"(Model memorized the training data too much)\")\n",
    "elif gap < -0.05:\n",
    "    print(\"Test performed better (Unusual but okay due to sampling)\")\n",
    "else:\n",
    "    print(\"Good Generalization (Not Overfitting)\")\n",
    "    print(\"(Model is robust and ready for production)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b72b2148-fee5-4d1a-a489-99897be7974b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FEATURE IMPORTANCE VISUALIZATION\n",
    "importances = pipeline_model.stages[-1].featureImportances\n",
    "feat_imp = pd.DataFrame(list(zip(feature_cols, importances.toArray())), \n",
    "                        columns=['Feature', 'Importance']).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=feat_imp, palette=\"magma\")\n",
    "plt.title('Why the Model thinks it is Fraud?', fontsize=14)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8851590791735036,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "paysim_project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}