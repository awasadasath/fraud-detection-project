{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc122c9c-90b1-4b1b-a10d-ef280f9e272f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PART 1: INGESTION\n",
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "# CONFIGURATION\n",
    "DBFS_KEY_PATH = \"dbfs:/FileStore/tables/gcp_key.json\" \n",
    "\n",
    "BUCKET_NAME = \"paysim-datalake\"\n",
    "FILE_NAME = \"paysim.csv\"\n",
    "\n",
    "# Path ชั่วคราวบน Driver (Local)\n",
    "LOCAL_KEY_PATH = \"/tmp/gcp_key.json\"\n",
    "LOCAL_DATA_PATH = f\"/tmp/{FILE_NAME}\"\n",
    "# Path บน DBFS (เพื่อให้ Spark อ่านได้)\n",
    "DBFS_DATA_PATH = f\"dbfs:/FileStore/raw/{FILE_NAME}\"\n",
    "\n",
    "print(\"Step 1: Secure Ingestion\")\n",
    "\n",
    "try:\n",
    "    # 1. Setup Credentials (Copy Key from DBFS -> Local)\n",
    "    dbutils.fs.cp(DBFS_KEY_PATH, f\"file:{LOCAL_KEY_PATH}\")\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = LOCAL_KEY_PATH\n",
    "    print(\"Credentials Loaded.\")\n",
    "\n",
    "    # 2. Download Data from GCS to Local Driver\n",
    "    print(f\"Downloading {FILE_NAME}\")\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "    blob = bucket.blob(FILE_NAME)\n",
    "    blob.download_to_filename(LOCAL_DATA_PATH)\n",
    "    \n",
    "    # 3. Move Data to DBFS (เพื่อให้ใช้ spark.read ได้)\n",
    "    dbutils.fs.mv(f\"file:{LOCAL_DATA_PATH}\", DBFS_DATA_PATH)\n",
    "    \n",
    "    # 4. Read with Spark\n",
    "    print(\"Reading CSV with Spark\")\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(DBFS_DATA_PATH)\n",
    "    \n",
    "    print(\"Ingestion successful.\")\n",
    "\n",
    "    # Preview Data\n",
    "    print(\"Previewing Data:\")\n",
    "    display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01de61e9-edcd-47f1-bae1-579107d2f77d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PART 2: BRONZE LAYER\n",
    "    print(\"Step 1: Creating Bronze Layer\")\n",
    "    \n",
    "    # 1. Check Schema\n",
    "    print(\"Schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    # 2. Count Rows\n",
    "    print(f\"Total Rows: {df.count():,}\")\n",
    "    \n",
    "    # 3. Save as Delta Table\n",
    "    print(\"Creating Bronze Table: 'paysim_bronze'\")\n",
    "    spark.sql(\"DROP TABLE IF EXISTS paysim_bronze\")\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"paysim_bronze\")\n",
    "    \n",
    "    print(\"Bronze Layer Created\")\n",
    "    \n",
    "    # Cleanup Local Key\n",
    "    if os.path.exists(LOCAL_KEY_PATH): os.remove(LOCAL_KEY_PATH)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    if os.path.exists(LOCAL_KEY_PATH): os.remove(LOCAL_KEY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b1172c-e45c-4d0a-97de-bbf2d919617c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PART 3: SILVER LAYER (CLEANSED & QUALITY CHECK)\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "print(\"Step 2: Creating Silver Layer\")\n",
    "\n",
    "# 1. Read from Bronze Delta\n",
    "df_bronze = spark.table(\"paysim_bronze\")\n",
    "\n",
    "# Step A: Data Quality Check\n",
    "# Rule: Amount must be non-negative. Negative values indicate system errors.\n",
    "dq_condition = col(\"amount\") >= 0\n",
    "\n",
    "# Separate Bad Data (Quarantine) from \"Valid Data\"\n",
    "df_valid_amount = df_bronze.filter(dq_condition)\n",
    "df_quarantine = df_bronze.filter(~dq_condition).withColumn(\"dq_issue\", lit(\"Negative Amount\"))\n",
    "\n",
    "# Step B: Business Scope Separation\n",
    "# Rule: We strictly focus on TRANSFER and CASH_OUT for fraud detection.\n",
    "# Other types (PAYMENT, DEBIT) are valid but out of scope for this model.\n",
    "scope_condition = col(\"type\").isin(\"TRANSFER\", \"CASH_OUT\")\n",
    "\n",
    "# 1. Target Data (Silver): Ready for ML processing\n",
    "df_silver = df_valid_amount.filter(scope_condition)\n",
    "\n",
    "# 2. Out-of-Scope Data (Others): Archived for future analytics\n",
    "df_others = df_valid_amount.filter(~scope_condition)\n",
    "\n",
    "# 4. Type Casting for Performance (Applied to Silver Table)\n",
    "# Casting String to Double/Integer for efficient calculation\n",
    "df_silver = df_silver.withColumn(\"amount\", col(\"amount\").cast(\"double\")) \\\n",
    "                   .withColumn(\"oldbalanceOrg\", col(\"oldbalanceOrg\").cast(\"double\")) \\\n",
    "                   .withColumn(\"newbalanceOrig\", col(\"newbalanceOrig\").cast(\"double\")) \\\n",
    "                   .withColumn(\"oldbalanceDest\", col(\"oldbalanceDest\").cast(\"double\")) \\\n",
    "                   .withColumn(\"newbalanceDest\", col(\"newbalanceDest\").cast(\"double\")) \\\n",
    "                   .withColumn(\"isFraud\", col(\"isFraud\").cast(\"integer\"))\n",
    "\n",
    "# 5. Save as Delta Tables\n",
    "print(\"Creating Silver Table (Target): 'paysim_silver'\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS paysim_silver\")\n",
    "df_silver.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"paysim_silver\")\n",
    "\n",
    "print(\"Creating Others Table (Out of Scope): 'paysim_others'\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS paysim_others\")\n",
    "df_others.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"paysim_others\")\n",
    "\n",
    "print(\"Creating Quarantine Table (Bad Data): 'paysim_quarantine'\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS paysim_quarantine\")\n",
    "df_quarantine.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"paysim_quarantine\")\n",
    "\n",
    "# Print Statistics\n",
    "print(f\"\"\"\n",
    "Stats Summary:\n",
    "- Silver (ML Ready):   {spark.table('paysim_silver').count():,} rows\n",
    "- Others (Ignored):    {spark.table('paysim_others').count():,} rows\n",
    "- Quarantine (Bad):    {spark.table('paysim_quarantine').count():,} rows\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "975f8dab-d645-4bff-9aca-47d6265b4be1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PART 4: GOLD LAYER (FEATURES & RISK)\n",
    "from pyspark.sql.functions import col, when, current_date, lit, max as spark_max\n",
    "\n",
    "print(\"Step 3: Creating Gold Layer\")\n",
    "\n",
    "# 1. Read from Silver Delta\n",
    "df_silver = spark.table(\"paysim_silver\")\n",
    "\n",
    "# A: Feature Engineering\n",
    "# We are adding 3 critical features for fraud detection:\n",
    "# 1. errorBalance: Detects mathematical anomalies in account balances.\n",
    "# 2. hourOfDay: Fraud often happens at specific hours (extracted from step).\n",
    "# 3. amountRatio: Fraudsters tend to empty accounts (amount / oldBalance).\n",
    "\n",
    "df_gold = df_silver.withColumn(\"errorBalanceOrig\", col(\"newbalanceOrig\") + col(\"amount\") - col(\"oldbalanceOrg\")) \\\n",
    "                   .withColumn(\"errorBalanceDest\", col(\"oldbalanceDest\") + col(\"amount\") - col(\"newbalanceDest\")) \\\n",
    "                   .withColumn(\"type_index\", when(col(\"type\") == \"TRANSFER\", 0).otherwise(1)) \\\n",
    "                   .withColumn(\"hourOfDay\", col(\"step\") % 24) \\\n",
    "                   .withColumn(\"amountRatio\", col(\"amount\") / (col(\"oldbalanceOrg\") + 0.001))\n",
    "\n",
    "# Select Final Columns for ML\n",
    "final_columns = [\n",
    "    \"step\", \n",
    "    \"type_index\", \n",
    "    \"amount\", \n",
    "    \"amountRatio\",      \n",
    "    \"hourOfDay\",      \n",
    "    \"oldbalanceOrg\", \n",
    "    \"newbalanceOrig\", \n",
    "    \"errorBalanceOrig\",\n",
    "    \"oldbalanceDest\", \n",
    "    \"newbalanceDest\", \n",
    "    \"errorBalanceDest\",\n",
    "    \"isFraud\"\n",
    "]\n",
    "\n",
    "df_gold_ml = df_gold.select(final_columns)\n",
    "\n",
    "print(\"Creating Gold Table: 'paysim_gold'\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS paysim_gold\")\n",
    "df_gold_ml.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"paysim_gold\")\n",
    "\n",
    "# B: Risk Profile (Customer Behavioral Snapshot)\n",
    "# Identify high-risk customers based on max transaction history.\n",
    "# Strategy: Daily Full Refresh (Snapshot) to update the latest status for ALL customers.\n",
    "\n",
    "df_risk_profile = df_silver.groupBy(\"nameOrig\") \\\n",
    "    .agg(spark_max(\"amount\").alias(\"max_txn_amount\")) \\\n",
    "    .withColumn(\"risk_level\", when(col(\"max_txn_amount\") > 1000000, \"High\").otherwise(\"Low\")) \\\n",
    "    .withColumn(\"effective_date\", current_date()) \\\n",
    "    .withColumn(\"is_current\", lit(True))\n",
    "\n",
    "# Write to Delta Lake (Overwrite mode = Always keep the latest state)\n",
    "# Note: We save ALL customers (Low & High) to maintain a complete Dimension Table, not just a blacklist.\n",
    "df_risk_profile.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dim_customer_risk\")\n",
    "\n",
    "# For demonstration, display only HIGH RISK customers\n",
    "print(\"Displaying Sample of HIGH RISK Customers:\")\n",
    "display(df_risk_profile.filter(col(\"risk_level\") == \"High\"))\n",
    "\n",
    "print(\"Gold Layer Created Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcbcf9ca-a8a8-4e34-959c-e1a7ef8ab661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PART 5: MACHINE LEARNING PIPELINE & EVALUATION\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Starting Optimized ML Pipeline\")\n",
    "\n",
    "# 1. Load Data from Delta\n",
    "try:\n",
    "    df_gold = spark.table(\"paysim_gold\")\n",
    "    gold_count = df_gold.count() \n",
    "    print(f\"Loaded Gold Data. Total Rows: {gold_count:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# STEP A: STRICT TIME-SERIES SPLIT\n",
    "# แยกอดีต (Train) ออกจากอนาคต (Test) ก่อนทำ Sampling\n",
    "split_point = df_gold.approxQuantile(\"step\", [0.8], 0.01)[0]\n",
    "train_raw = df_gold.filter(col(\"step\") <= split_point)\n",
    "test_raw = df_gold.filter(col(\"step\") > split_point)\n",
    "\n",
    "# STEP B: STRATEGIC SAMPLING\n",
    "# Train: เก็บ Fraud ครบ + สุ่ม Normal เหลือ 5%\n",
    "train_data = train_raw.filter(col(\"isFraud\") == 1).union(\n",
    "    train_raw.filter(col(\"isFraud\") == 0).sample(False, 0.05, seed=1234)\n",
    ")\n",
    "\n",
    "# Test: เก็บ Fraud ครบ + สุ่ม Normal เหลือ 5%\n",
    "test_data = test_raw.filter(col(\"isFraud\") == 1).union(\n",
    "    test_raw.filter(col(\"isFraud\") == 0).sample(False, 0.05, seed=5678)\n",
    ")\n",
    "\n",
    "print(f\"Training Set: {train_data.count():,} rows\")\n",
    "print(f\"Test Set: {test_data.count():,} rows\")\n",
    "\n",
    "# ใช้ Pipeline รวมขั้นตอนการทำ Feature และ Model เข้าด้วยกัน\n",
    "feature_cols = [\n",
    "    \"type_index\", \"amount\", \"amountRatio\", \"hourOfDay\",\n",
    "    \"oldbalanceOrg\", \"newbalanceOrig\", \"errorBalanceOrig\",\n",
    "    \"oldbalanceDest\", \"newbalanceDest\", \"errorBalanceDest\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"isFraud\", \n",
    "    featuresCol=\"features\", \n",
    "    numTrees=10,\n",
    "    maxDepth=7, \n",
    "    seed=1234\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# TRAINING\n",
    "print(\"Training Random Forest via Pipeline\")\n",
    "pipeline_model = pipeline.fit(train_data)\n",
    "print(\"Training Complete!\")\n",
    "\n",
    "# EVALUATION\n",
    "predictions = pipeline_model.transform(test_data)\n",
    "\n",
    "# คำนวณค่าเฉพาะ Class 1 (Fraud)\n",
    "results = predictions.groupBy(\"isFraud\", \"prediction\").count().collect()\n",
    "tp = 0; fp = 0; fn = 0\n",
    "for row in results:\n",
    "    if row['isFraud'] == 1 and row['prediction'] == 1.0: tp = row['count']\n",
    "    elif row['isFraud'] == 0 and row['prediction'] == 1.0: fp = row['count']\n",
    "    elif row['isFraud'] == 1 and row['prediction'] == 0.0: fn = row['count']\n",
    "\n",
    "# Metrics Summary\n",
    "try:\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"isFraud\", predictionCol=\"prediction\")\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "except:\n",
    "    precision = recall = f1 = accuracy = 0\n",
    "\n",
    "print(f\"FINAL PERFORMANCE\")\n",
    "print(\"=\"*45)\n",
    "print(f\" Accuracy:  {accuracy:.4f}\")\n",
    "print(f\" Precision: {precision:.2%}\")\n",
    "print(f\" Recall:    {recall:.2%}\")\n",
    "print(f\" F1-Score:  {f1:.4f}\")\n",
    "print(\"-\" * 45)\n",
    "print(f\" Caught Fraud (TP): {tp}\")\n",
    "print(f\" Missed Fraud (FN): {fn}\")\n",
    "print(f\" False Alarm (FP):  {fp}\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# แสดง Confusion Matrix\n",
    "predictions.groupBy(\"isFraud\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b72b2148-fee5-4d1a-a489-99897be7974b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FEATURE IMPORTANCE VISUALIZATION\n",
    "importances = pipeline_model.stages[-1].featureImportances\n",
    "feat_imp = pd.DataFrame(list(zip(feature_cols, importances.toArray())), \n",
    "                        columns=['Feature', 'Importance']).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=feat_imp, palette=\"magma\")\n",
    "plt.title('Why the Model thinks it is Fraud?', fontsize=14)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8851590791735036,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "paysim_project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}